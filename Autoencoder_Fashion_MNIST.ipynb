{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we will see how we can use autoencoder as a classifier with fashion MNIST dataset.\n",
    "- Fashion MNIST dataset is a 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images, and the test set has 10,000 images.\n",
    "- It is a replacement for the original MNIST handwritten digits dataset for producing better results. The Fashion MNIST dataset consists of 10 different classes of fashion accessories like shirts, trousers, sandals, etc \n",
    "- The image dimensions, training and test splits are similar to the original MNIST dataset. \n",
    "- The dataset is freely available on this URL and can be loaded using both tensorflow and keras as a framework without having to download it on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we want to do is built a convolutional autoencoder and use the encoder part of it combined with fully connected layers to recognize a new sample from the test set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gzip\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Adadelta\n",
    "from keras.layers import Conv2D, Input, Dense, Flatten,Dropout, merge, Reshape, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras import backend as k\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we read the data is as follows:\n",
    "- We open and read the gzip file as bytestream\n",
    "- We then convert the string stored in buffer using np.frombuffer() to numpy array of type float32.\n",
    "- Data is then reshaped into a 3-dimentional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buffer = bytestream.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(buffer, dtype = np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, 28,28)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extract_data('train-images-idx3-ubyte.gz' ,60000)\n",
    "test_data = extract_data('t10k-images-idx3-ubyte.gz',10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we read the labels is the same as we read the data:\n",
    "- We define a function which opens the label file as a bytestream using bytestream.read() to which you pass the label dimension as 1 and the total number of images.\n",
    "- We then convert the string stored in the buffer to a numpy array of type int64.\n",
    "- We do not need to reshape the array since the variable labels will return a column vector of dimension 60,000 x 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buffer = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buffer, dtype = np.uint8).astype(np.uint64)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = extract_labels('train-labels-idx1-ubyte.gz', 60000)\n",
    "test_labels = extract_labels('t10k-labels-idx1-ubyte.gz',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (images) shape: {shape} (60000, 28, 28)\n",
      "Test set (image) shape: {shape} (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# To see the dimension of images in the training set\n",
    "print(\"Training set (images) shape: {shape}\", format(train_data.shape))\n",
    "\n",
    "# To see the dimension of images in the test set\n",
    "print(\"Test set (image) shape: {shape}\", format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need training an testing labels for convolutional encoder as the task at hand is to reconstruct the images. However, we do need them for classification task and for data exploration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now create a dictionary of class names with their corresponding class labels:\n",
    "label_dict = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "    3: 'D',\n",
    "    4: 'E',\n",
    "    5: 'F',\n",
    "    6: 'G',\n",
    "    7: 'H',\n",
    "    8: 'I',\n",
    "    9: 'J' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at few of the images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'(Label: E)')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65ec6a84a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# display an image from the training data\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(train_data[10], (28,28))\n",
    "curr_label = train_labels[10]\n",
    "plt.imshow(curr_img, cmap = 'gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_label]) + \")\")\n",
    "\n",
    "# display an image from the testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(test_data[10], (28,28))\n",
    "curr_label = test_labels[10]\n",
    "plt.imshow(curr_img, cmap= 'gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_label])+ \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are assigned class labels as 0 or A / 4 or E. This means all the alphabets having class of E will have the class label of 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do the preprocessing of the data before feeding it to the model as our images are gray scale images having pixel values ranging from 0 to 255 with dimension of 28 * 28.\n",
    "So we will convert each 28 * 28 image into a matrix of size 28 * 28 * 1 which can be fed to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape (60000, 28, 28, 1)\n",
      "Test data shape (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.reshape(-1, 28, 28, 1)\n",
    "test_data = test_data.reshape(-1, 28, 28, 1)\n",
    "print(\"Train data shape\", train_data.shape)\n",
    "print(\"Test data shape\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the datatype of the numpy arrays\n",
    "train_data.dtype, test_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling the pixel values to 0-1 scale by rescaling with maximum pixel value of the training and testing data\n",
    "train_data = train_data / np.max(train_data)\n",
    "test_data = test_data / np.max(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the maximum value of the training and the testing data\n",
    "np.max(train_data), np.max(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now split the data into 80% training and 20% validation. This will help us to reduce the chances of overfitting\n",
    "# as you would be validating the model on the data it has not seen in the training.\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_data, test_size = 0.2, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 28, 28, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 28, 28, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the data is ready to be fed to the network. \n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "channel = 1\n",
    "x, y = 28, 28\n",
    "input_img = Input(shape = (x, y, channel))\n",
    "num_classes = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
